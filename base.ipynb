{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.models import detection\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "from tempfile import TemporaryDirectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in C:/Users/David/PycharmProjects/reliableML/data/imagenet/train.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 37\u001B[0m\n\u001B[0;32m     28\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# they used ImageNet as the base dataset\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# they used ImageNet-V2, ImageNet-VidRobust, ImageNet-Rendition and ImageNet-Sketch for ImageNet-Adversarial for natural OOD shift. They also used ImageNet-Adversarial as a natural OOD shift. This dataset was examined separately\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# they used ImageNet-C as a synthetic OOD shift\u001B[39;00m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;66;03m# They also somehow used ImageNet-Validation\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m trainset \u001B[38;5;241m=\u001B[39m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mImageNet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     38\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mdownload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_transforms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m trainloader \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataLoader(trainset, batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m     40\u001B[0m                                           shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     42\u001B[0m valset \u001B[38;5;241m=\u001B[39m torchvision\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mImageNet(root\u001B[38;5;241m=\u001B[39mval_dir, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     43\u001B[0m                                        download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mdata_transforms)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:46\u001B[0m, in \u001B[0;36mImageNet.__init__\u001B[1;34m(self, root, split, **kwargs)\u001B[0m\n\u001B[0;32m     43\u001B[0m root \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexpanduser(root)\n\u001B[0;32m     44\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit \u001B[38;5;241m=\u001B[39m verify_str_arg(split, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit\u001B[39m\u001B[38;5;124m\"\u001B[39m, (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mval\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse_archives\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m wnid_to_classes \u001B[38;5;241m=\u001B[39m load_meta_file(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     49\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_folder, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:59\u001B[0m, in \u001B[0;36mImageNet.parse_archives\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mparse_archives\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m check_integrity(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mroot, META_FILE)):\n\u001B[1;32m---> 59\u001B[0m         \u001B[43mparse_devkit_archive\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit_folder):\n\u001B[0;32m     62\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msplit \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:140\u001B[0m, in \u001B[0;36mparse_devkit_archive\u001B[1;34m(root, file)\u001B[0m\n\u001B[0;32m    137\u001B[0m     file \u001B[38;5;241m=\u001B[39m archive_meta[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    138\u001B[0m md5 \u001B[38;5;241m=\u001B[39m archive_meta[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m--> 140\u001B[0m \u001B[43m_verify_archive\u001B[49m\u001B[43m(\u001B[49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmd5\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_tmp_dir() \u001B[38;5;28;01mas\u001B[39;00m tmp_dir:\n\u001B[0;32m    143\u001B[0m     extract_archive(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(root, file), tmp_dir)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:96\u001B[0m, in \u001B[0;36m_verify_archive\u001B[1;34m(root, file, md5)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m check_integrity(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(root, file), md5):\n\u001B[0;32m     92\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     93\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe archive \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m is not present in the root directory or is corrupted. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     94\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou need to download it externally and place it in \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     95\u001B[0m     )\n\u001B[1;32m---> 96\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg\u001B[38;5;241m.\u001B[39mformat(file, root))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The archive ILSVRC2012_devkit_t12.tar.gz is not present in the root directory or is corrupted. You need to download it externally and place it in C:/Users/David/PycharmProjects/reliableML/data/imagenet/train."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Some code is from here: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "# maybe we need to change the transformations. Maybe the model weights were calculated using data with different transformations. Or maybe we need to finetune each model to our transformations\n",
    "\n",
    "if 'David' in os.getcwd():  # checking in whose computer we are\n",
    "    train_dir = \"C:/Users/David/PycharmProjects/reliableML/data/imagenet/train\"\n",
    "    val_dir = 'C:/Users/David/PycharmProjects/reliableML/data/imagenet/val'\n",
    "else:\n",
    "    print(\"The path of the files is in Amitay's computer\")\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# imagenet_data = torchvision.datasets.ImageNet('path/to/imagenet_root/')\n",
    "\n",
    "# they used ImageNet as the base dataset\n",
    "# they used ImageNet-V2, ImageNet-VidRobust, ImageNet-Rendition and ImageNet-Sketch for ImageNet-Adversarial for natural OOD shift. They also used ImageNet-Adversarial as a natural OOD shift. This dataset was examined separately\n",
    "# they used ImageNet-C as a synthetic OOD shift\n",
    "# They also somehow used ImageNet-Validation\n",
    "\n",
    "trainset = torchvision.datasets.ImageNet(root=train_dir, train=True,\n",
    "                                        download=True, transform=data_transforms)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "valset = torchvision.datasets.ImageNet(root=val_dir, train=False,\n",
    "                                       download=True, transform=data_transforms)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# not really our classes so should be changed or deleted\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "image_datasets = {'train': trainset,\n",
    "                  'test': valset}\n",
    "dataloaders = {'train': trainloader,\n",
    "                  'test': valloader}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# the models that were used in the paper: resnet18, resnet34, resnet50, resnet101, resnet152, vgg19, alexnet, resnext101_32x8d or resnext101_64x4d (they didn't specify which resnet101 they used. Also used wide_resnet101_2 and AugMix, DeepAugment, AM-DeepAugment and Deep Ensembles\n",
    "resnet18 = models.resnet18(weights='IMAGENET1K_V1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    # The function does training including validation\n",
    "    # it's taken from here:\n",
    "    # https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#training-the-model\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def AC(model, dataset):\n",
    "    # the function does inference and then calculates the average confidence of the model on the dataset\n",
    "\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(trainset, batch_size=len(dataset),\n",
    "                                          shuffle=True, num_workers=2)  # a dataloader that loads all the samples at once\n",
    "    inputs, _ = next(iter(loader))  # all samples without labels\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        probmax = torch.max(outputs, axis = 0)  # the score(s) of the class with the highest score(s)\n",
    "        average_confidence = torch.mean(probmax)  # calculating average confidence\n",
    "\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "    return average_confidence\n",
    "\n",
    "\n",
    "def DOC(model, in_distribution_dataset, out_distribution_dataset):\n",
    "    # the function calculates the average confidence of the model on two datasets and then calculates the DoC (difference of confidences) over the two datasets\n",
    "\n",
    "    DoC = AC(model, in_distribution_dataset) - AC(model, out_distribution_dataset)\n",
    "    return DoC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ft = resnet18\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# I found an implementation of DoC but they did a bit different from what I did. They subtracted the accuracy from the average confidence and said that the result is the DoC. I don't know why: https://github.com/ZerojumpLine/ModelEvaluationUnderClassImbalance/blob/main/Prostate.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
